{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9224011-def5-40c9-8e88-a1adfe3b347d",
   "metadata": {},
   "source": [
    "# 在Sagemaker上训练和部署Llama 2 (7B - 70B)\n",
    "\n",
    "- SageMaker 与 Hugging Face 深度合作，通过 JumpStart 一站式的 Portal 平台提供众多模型的一键集成和部署服务\n",
    "- SageMaker 使用流行的开源库维护深度学习容器（DLC），用于在 AWS 基础设施上托管大型模型，例如 GPT、T5、OPT、BLOOM 和 Stable Diffusion。借助这些 DLC，您可以使用 DeepSpeed、Accelerate 和 FasterTransformer 等第三方库，使用模型并行技术对模型参数进行分区，以利用多个 GPU 的内存进行推理。\n",
    "\n",
    "Main Steps:\n",
    "1. Setup Development Environment\n",
    "2. Load and prepare the dataset\n",
    "3. Fine-Tune LLaMA 13B with QLoRA on Amazon SageMaker\n",
    "4. Deploy Fine-tuned LLM on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9eac89-c377-4fc4-8ed5-f579e686020b",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f635eb2e-aaa3-4612-8f42-d20a4c1dcf6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 注意sagemaker是否是最新的版本\n",
    "!pip install \"transformers==4.31.0\" \"datasets[s3]==2.13.0\" sagemaker --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81effc40-c2d5-4ed7-a937-758fa6531d9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install \"sagemaker>=2.175.0\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47087a55-e28e-4f01-8bbc-78fa19e55e58",
   "metadata": {},
   "source": [
    "定义一个sagemaker session，默认使用的s3存储桶，IAM role等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad393869-4bc4-4063-8f27-3ccb3cd90db2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "\n",
    "# you can use your own bucket name\n",
    "sagemaker_session_bucket=None\n",
    "\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "# sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdf8ce1-09ef-4221-a53b-3ba4e14c928e",
   "metadata": {},
   "source": [
    "## Fine-tune LLaMA 2 on Amazon SageMaker\n",
    "optimizations ways for efficient fine-tuning:\n",
    "\n",
    "- Low-Rank Adaptation (LoRA) – This is a type of parameter efficient fine-tuning (PEFT) for efficient fine-tuning of large models. In this, we freeze the whole model and only add a small set of adjustable parameters or layers into the model. For instance, instead of training all 7 billion parameters for Llama 2 7B, we can fine-tune less than 1% of the parameters. This helps in significant reduction of the memory requirement because we only need to store gradients, optimizer states, and other training-related information for only 1% of the parameters. Furthermore, this helps in reduction of training time as well as the cost. For more details on this method, refer to LoRA: Low-Rank Adaptation of Large Language Models.\n",
    "- Int8 quantization – Even with optimizations such as LoRA, models such as Llama 70B are still too big to train. To decrease the memory footprint during training, we can use Int8 quantization during training. Quantization typically reduces the precision of the floating point data types. Although this decreases the memory required to store model weights, it degrades the performance due to loss of information. Int8 quantization uses only a quarter precision but doesn’t incur degradation of performance because it doesn’t simply drop the bits. It rounds the data from one type to the another. To learn about Int8 quantization, refer to LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale.\n",
    "- Fully Sharded Data Parallel (FSDP) – This is a type of data-parallel training algorithm that shards the model’s parameters across data parallel workers and can optionally offload part of the training computation to the CPUs. Although the parameters are sharded across different GPUs, computation of each microbatch is local to the GPU worker. It shards parameters more uniformly and achieves optimized performance via communication and computation overlapping during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699ff2ad-b4d5-413a-8744-f999e63a09b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 数据集准备示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4115a781-1ad5-4c28-aa03-cfb91c1b0d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dolly_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "# To train for question answering/information extraction, you can replace the assertion in next line to example[\"category\"] == \"closed_qa\"/\"information_extraction\".\n",
    "summarization_dataset = dolly_dataset.filter(lambda example: example[\"category\"] == \"summarization\")\n",
    "summarization_dataset = summarization_dataset.remove_columns(\"category\")\n",
    "\n",
    "# We split the dataset into two where test data is used to evaluate at the end.\n",
    "train_and_test_dataset = summarization_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# Dumping the training data to a local file to be used for training.\n",
    "train_and_test_dataset[\"train\"].to_json(\"train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eef1a2a-ef58-411f-ac95-3fdab22d5f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617fd2cd-9168-498f-8a38-d8df91da6491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reate a prompt template\n",
    "import json\n",
    "\n",
    "template = {\n",
    "    \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\\n\\n\",\n",
    "    \"completion\": \" {response}\",\n",
    "}\n",
    "with open(\"template.json\", \"w\") as f:\n",
    "    json.dump(template, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a69575a-9688-4f8b-aea3-41dccc48f34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload to s3\n",
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import random\n",
    "\n",
    "output_bucket = sagemaker.Session().default_bucket()\n",
    "local_data_file = \"train.jsonl\"\n",
    "train_data_location = f\"s3://{output_bucket}/dolly_dataset\"\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "S3Uploader.upload(\"template.json\", train_data_location)\n",
    "print(f\"Training data: {train_data_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cca5494-fad2-43f5-b23c-9994f6470317",
   "metadata": {},
   "source": [
    "### 训练实例类型选择参考：\n",
    "\n",
    "\n",
    "| Model        | Instance Type     | Max Batch Size | Context Length |\n",
    "|--------------|-------------------|----------------|----------------|\n",
    "| [LLama 7B]() | `(ml.)g5.4xlarge` | `3`            | `2048`         |\n",
    "| [LLama 13B]() | `(ml.)g5.4xlarge` | `2`            | `2048`         |\n",
    "| [LLama 70B]() | `(ml.)p4d.24xlarge` | `1++` (need to test more configs)            | `2048`         |\n",
    "\n",
    "\n",
    "> You can also use `g5.2xlarge` instead of the `g5.4xlarge` instance type, but then it is not possible to use `merge_weights` parameter, since to merge the LoRA weights into the model weights, the model needs to fit into memory. But you could save the adapter weights and merge them using [merge_adapter_weights.py](./scripts/merge_adapter_weights.py) after training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9421280-6fd9-41cc-9897-fa910c1a0a4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fine-tune via the Hugging Face SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256331d9-b79f-44ac-9168-9b0b98edf8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-qlora-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                             # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training',    # path where sagemaker will save training dataset\n",
    "  'epochs': 3,                                      # number of training epochs\n",
    "  'per_device_train_batch_size': 2,                 # batch size for training\n",
    "  'lr': 2e-4,                                       # learning rate used during training\n",
    "  'hf_token': HfFolder.get_token(),                 # huggingface token to access llama 2\n",
    "  'merge_weights': True,                            # wether to merge LoRA into the model (needs more memory)\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_clm.py',      # train script\n",
    "    source_dir           = 'scripts',         # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.4xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa25b44f-a0f2-4296-8243-ae2bf942f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6f78cd-57aa-4add-a0b9-37b9a0e39a06",
   "metadata": {},
   "source": [
    "### Fine-tune via the SageMaker Python SDK\n",
    "more training  [sample code](https://github.com/aws/amazon-sagemaker-examples/tree/5daada592797be296c1aa7e7964e73900473dddd/introduction_to_amazon_algorithms/jumpstart-foundation-models#6.-Studio-Kernel-Dead/Creating-JumpStart-Model-from-the-training-Job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ac476b-0d4d-49b8-8041-4270d85bebf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "model_id, model_version = \"meta-textgeneration-llama-2-7b\", \"*\"\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    environment={\"accept_eula\": \"true\"},\n",
    "    disable_output_compression=True,  # For Llama-2-70b, add instance_type = \"ml.g5.48xlarge\"\n",
    ")\n",
    "# By default, instruction tuning is set to false. Thus, to use instruction tuning dataset you use\n",
    "estimator.set_hyperparameters(instruction_tuned=\"True\", epoch=\"5\", max_input_length=\"1024\")\n",
    "estimator.fit({\"training\": train_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b36cd4-717f-448a-8008-799f258935c1",
   "metadata": {},
   "source": [
    "Find the training job name:\n",
    "Go to Console -> SageMaker -> Training -> Training Jobs -> Identify the training job name and substitute in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a9958f-d99f-4b6d-be9c-d81246051f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果kernel 断开之后通过training jod 新建estimator对象\n",
    "'''\n",
    "# from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "# training_job_name = <<training_job_name>>\n",
    "\n",
    "# estimator = JumpStartEstimator.attach(training_job_name, model_id)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5554ca28-3136-4d55-8fe9-ec141cee9bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93c82f7c-31a0-4a64-ba8f-b3bf76189696",
   "metadata": {},
   "source": [
    "## Deploy Llama 2 (7B - 70B) on Amazon SageMaker\n",
    "如果直接在Sagemaker上训练模型的话部署会比较简单，使用以下方式可以直接部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a14e9c40-084d-4812-9c1b-2f4914d1948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you train llama on sagemaker,you can deploy with the estimator defined before\n",
    "'''\n",
    "finetuned_predictor = estimator.deploy()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8c6e8c-2a71-4aa4-95f0-60dec2354ecd",
   "metadata": {},
   "source": [
    "如果直接拉取hf上的模型，部署方式如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cacf2b6-d63a-4020-9cbc-a72da2f33189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy pre-trained model with jumpstart sdk\n",
    "'''\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "pretrained_model = JumpStartModel(model_id=model_id)\n",
    "pretrained_predictor = pretrained_model.deploy()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aea0ec87-c319-47f3-a039-9b8e975044c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# deploy pre-trained model with hugging face sdk\n",
    "'''\n",
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.p4d.24xlarge\"\n",
    "number_of_gpu = 8\n",
    "health_check_timeout = 300\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"meta-llama/Llama-2-70b-chat-hf\", # model_id from hf.co/models\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(2048),  # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(4096),  # Max length of the generation (including input text)\n",
    "  'MAX_BATCH_TOTAL_TOKENS': json.dumps(8192),  # Limits the number of tokens that can be processed in parallel during the generation\n",
    "  'HUGGING_FACE_HUB_TOKEN': \"<REPLACE WITH YOUR TOKEN>\"\n",
    "  # ,'HF_MODEL_QUANTIZE': \"bitsandbytes\", # comment in to quantize\n",
    "}\n",
    "\n",
    "# check if token is set\n",
    "assert config['HUGGING_FACE_HUB_TOKEN'] != \"<REPLACE WITH YOUR TOKEN>\", \"Please set your Hugging Face Hub token\"\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  env=config\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbccd37-7e89-45d1-8ca0-b561b7f4a0f0",
   "metadata": {},
   "source": [
    "Main Steps：\n",
    "1. #Setup development environment\n",
    "2. Retrieve LLM DLC\n",
    "3. Hardware requirements\n",
    "4. Create your own model and deploy\n",
    "5. Run inference and chat with the mode\n",
    "6. Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea06e51-37d1-4e20-9dc8-aa8d10aa7d6c",
   "metadata": {},
   "source": [
    "### 自定义推理镜像\n",
    "如果是已经训练好的模型只需要做部署的话，首先需要指定推理镜像，接下来推荐两个方式：\n",
    "- hugging facing\n",
    "- sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "550cfbbc-3d8a-45bc-9ce9-3d52f93070fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "llm image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi0.9.3-gpu-py39-cu118-ubuntu20.04\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"0.9.3\"\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585bb40f-aec8-465d-8758-3ccd9fcae306",
   "metadata": {},
   "source": [
    "除了上面的方式也可以直接指定镜像来做推理镜像。\n",
    "Sagemaker 可用的镜像参考：[docu](https://docs.aws.amazon.com/zh_cn/sagemaker/latest/dg/large-model-inference-dlc.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "736ed132-d1fb-4ee5-80ac-47b5504001fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image going to be used is ---- &gt; 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.23.0-deepspeed0.9.5-cu118\n"
     ]
    }
   ],
   "source": [
    "# inference_image_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/djl-ds:latest\"\n",
    "region = sess.boto_region_name\n",
    "inference_image_uri_from_sm= (\n",
    "    f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.23.0-deepspeed0.9.5-cu118\"\n",
    ")\n",
    "print(f\"Image going to be used is ---- &gt; {inference_image_uri_from_sm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53c52d1-30a0-48f1-8121-cf0995dd1005",
   "metadata": {
    "tags": []
   },
   "source": [
    "Provide Serve Llama2-70B on SageMaker using DJL container [sample code](https://docs.djl.ai/docs/demos/aws/sagemaker/large-model-inference/sample-llm/rollingbatch_deploy_llama2_70b_w_pagedattn.html#create-the-model) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0253a487-4e4b-42bd-a6f4-8ba337d89203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76de609b-cf03-49bf-a523-1313772d13d3",
   "metadata": {},
   "source": [
    "### 部署实例选择参考\n",
    "更多关于大模型推断的计算资源计算可以参考 [文档](https://docs.aws.amazon.com/zh_cn/sagemaker/latest/dg/large-model-inference-choosing-instance-types.html)\n",
    "| Model        | Instance Type     | Quantization | # of GPUs per replica | \n",
    "|--------------|-------------------|--------------|-----------------------|\n",
    "| [Llama 7B](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) | `(ml.)g5.2xlarge` | `-`          | 1 |\n",
    "| [Llama 13B](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf) | `(ml.)g5.12xlarge` | `-`          | 4                     | \n",
    "| [Llama 70B](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf) | `(ml.)g5.48xlarge` | `bitsandbytes`          | 8                     | \n",
    "| [Llama 70B](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf) | `(ml.)p4d.24xlarge` | `-`          | 8                     | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d8681c-4340-4a23-984c-ecdb56a6bd85",
   "metadata": {},
   "source": [
    "### 部署模型\n",
    "部署 ML 模型时，一种选择是将模型工件存档并压缩为某种tar.gz格式。尽管此方法适用于小型模型，但压缩具有数千亿个参数的大型模型工件，然后在端点上对其进行解压缩可能会花费大量时间。对于大型模型推断，也可以直接部署未压缩的 ML 模型。\n",
    "接下来会将两种方式的部署都给出示例代码，可以根据您已经训练完的模型的格式选择一种方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3026c17b-3abc-4b1b-989f-ad239bd643c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# deploy with sagemaker python sdk boto3\n",
    "\n",
    "import boto3\n",
    "import json \n",
    "import sagemaker\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad55105-f1a6-4141-b3f4-f3791f4aa4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"define-by-yourself\"\n",
    "\n",
    "# define role with arn\n",
    "sagemaker_role = \"arn:aws:iam::123456789012:role/SageMakerExecutionRole\"\n",
    "# sagemaker_role = role \n",
    "\n",
    "# define container with arn or before\n",
    "'''\n",
    "image_uri = \"123456789012.dkr.ecr.us-west-2.amazonaws.com/inference-image:latest\"\n",
    "image_uri = llm_image\n",
    "'''\n",
    "image_uri = inference_image_uri_from_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c63995-41eb-44ac-9807-df1dd1ecfe9a",
   "metadata": {},
   "source": [
    "#### 将自己的模型注册到SageMaker Model中\n",
    "如果训练好的模型文件已经是tar.gz格式，使用ModelDataUrl指定保存模型文件所在的S3路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c27d61-a5fa-4124-8a0d-30c0e60c695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model_response = sm_client.create_model(\n",
    "    ModelName = model_name, \n",
    "    ExecutionRoleArn = sagemaker_role, \n",
    "    PrimaryContainer={\n",
    "        'Image': image_uri,\n",
    "        'ModelDataUrl': model_uri,\n",
    "        'Environment': {'MODEL_LOADING_TIMEOUT': '3600'}\n",
    "    })\n",
    "\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ad444e-8cad-4a51-8412-e4a00867d665",
   "metadata": {
    "tags": []
   },
   "source": [
    "如果需要直接部署未压缩的大模型，使用ModelDataSource构造一个DataSource对象，可以指定s3的一个文件夹路径："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5437cf7c-a814-436c-b5d2-a2e690ac070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = sagemaker_role,\n",
    "    PrimaryContainer = {\n",
    "        \"Image\": image_uri,\n",
    "        \"ModelDataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"S3Uri\": \"s3://my-bucket/prefix/to/model/data/\", \n",
    "                \"S3DataType\": \"S3Prefix\",\n",
    "                \"CompressionType\": \"None\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e118222-2754-480e-b084-c7dcf33361fe",
   "metadata": {},
   "source": [
    "#### 创建终端节点配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb106ab8-6c5a-4e42-9a03-49c23f2a4b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.p4d.24xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": 3600,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 3600,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca8148d-9f10-4148-841c-d54930d326f9",
   "metadata": {},
   "source": [
    "#### 部署模型\n",
    "该步骤可能会等待比较长的时间，等待endpoint的状态变成in service的状态大概20分钟左右"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b667e125-85d5-42c8-aa53-c093293be765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy model to an endpoint\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3c45b3-c591-463b-b48c-3d8d2e961626",
   "metadata": {},
   "source": [
    "以下代码可以查询endpoint部署状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1311737f-8f47-4bbd-9cec-998aa7eefc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041b97fe-f23e-42b3-85de-d3f21ac79f2d",
   "metadata": {},
   "source": [
    "### 测试部署好的节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baef4cd5-69bd-40cb-8ba0-f79cf081f687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_llama2_prompt(messages):\n",
    "    startPrompt = \"<s>[INST] \"\n",
    "    endPrompt = \" [/INST]\"\n",
    "    conversation = []\n",
    "    for index, message in enumerate(messages):\n",
    "        if message[\"role\"] == \"system\" and index == 0:\n",
    "            conversation.append(f\"<<SYS>>\\n{message['content']}\\n<</SYS>>\\n\\n\")\n",
    "        elif message[\"role\"] == \"user\":\n",
    "            conversation.append(message[\"content\"].strip())\n",
    "        else:\n",
    "            conversation.append(f\" [/INST] {message['content'].strip()} </s><s>[INST] \")\n",
    "\n",
    "    return startPrompt + \"\".join(conversation) + endPrompt\n",
    "  \n",
    "messages = [\n",
    "  { \"role\": \"system\",\"content\": \"You are a friendly and knowledgeable vacation planning assistant named Clara. Your goal is to have natural conversations with users to help them plan their perfect vacation. \"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9a9a4b-95cd-4785-8bb4-38b0844021d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define question and add to messages\n",
    "instruction = \"What are some cool ideas to do in the summer?\"\n",
    "messages.append({\"role\": \"user\", \"content\": instruction})\n",
    "prompt = build_llama2_prompt(messages)\n",
    "\n",
    "\n",
    "chat = llm.predict({\"inputs\":prompt})\n",
    "\n",
    "print(chat[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd12e30-8d81-4c36-86ff-fd6323a39f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for llm\n",
    "payload = {\n",
    "  \"inputs\":  prompt,\n",
    "  \"parameters\": {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "    \"stop\": [\"</s>\"]\n",
    "  }\n",
    "}\n",
    "\n",
    "# send request to endpoint\n",
    "response = llm.predict(payload)\n",
    "\n",
    "print(response[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c250d219-6cf4-47d8-b571-954f4f047003",
   "metadata": {},
   "source": [
    "### 使用Python调用SageMaker端点\n",
    "invoke_endpoint方法会发送HTTP请求到指定的端点,并返回响应。重要参数有:\n",
    "- EndpointName: 要调用的端点名称\n",
    "- Body: 要发送的数据\n",
    "- ContentType: Body数据的内容类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "296e95c5-8e2d-496d-9c5f-9670b04c263b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf40a032-9039-4216-bead-2f29ef605ac4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a low-level client representing Amazon SageMaker Runtime\n",
    "sagemaker_client = boto3.client('sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb8f77b-9d04-4d5f-9f49-59f0fed5d3ec",
   "metadata": {},
   "source": [
    "Console -> SageMaker -> Inference -> Endpoint -> find the in service endpoint name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefcef54-d688-47e8-881c-2432eca41853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the name of your LLM endpoint. \n",
    "endpoint_name='jumpstart-dft-meta-textgeneration-llama-2-70b-f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e14115-7c63-4b3d-a188-1f1f2c102209",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sagemaker_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=payload,\n",
    "    ContentType='application/json'\n",
    ")\n",
    "# CustomAttributes=\"accept_eula=true\"\n",
    "\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "print(result) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01884cf-cae2-41d5-b667-d39537fb4b41",
   "metadata": {},
   "source": [
    "## 测试结束需要及时删除推理节点\n",
    "Go to Console -> SageMaker -> Inference -> Endpoint -> Identify the test endpoint name and then del it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
