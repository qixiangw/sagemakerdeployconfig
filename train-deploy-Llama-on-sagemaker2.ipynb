{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9224011-def5-40c9-8e88-a1adfe3b347d",
   "metadata": {},
   "source": [
    "# 在Sagemaker上训练和部署Llama 2 (7B - 70B)\n",
    "\n",
    "- SageMaker 与 Hugging Face 深度合作，通过 JumpStart 一站式的 Portal 平台提供众多模型的一键集成和部署服务\n",
    "- SageMaker 使用流行的开源库维护深度学习容器（DLC），用于在 AWS 基础设施上托管大型模型，例如 GPT、T5、OPT、BLOOM 和 Stable Diffusion。借助这些 DLC，您可以使用 DeepSpeed、Accelerate 和 FasterTransformer 等第三方库，使用模型并行技术对模型参数进行分区，以利用多个 GPU 的内存进行推理。\n",
    "\n",
    "Main Steps:\n",
    "1. Setup Development Environment\n",
    "2. Load and prepare the dataset\n",
    "3. Fine-Tune LLaMA 13B with QLoRA on Amazon SageMaker\n",
    "4. Deploy Fine-tuned LLM on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9eac89-c377-4fc4-8ed5-f579e686020b",
   "metadata": {},
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f635eb2e-aaa3-4612-8f42-d20a4c1dcf6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 注意sagemaker是否是最新的版本\n",
    "!pip install \"transformers==4.31.0\" \"datasets[s3]==2.13.0\" sagemaker --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81effc40-c2d5-4ed7-a937-758fa6531d9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install \"sagemaker>=2.175.0\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47087a55-e28e-4f01-8bbc-78fa19e55e58",
   "metadata": {},
   "source": [
    "定义一个sagemaker session，默认使用的s3存储桶，IAM role等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad393869-4bc4-4063-8f27-3ccb3cd90db2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::249517808360:role/NotebookStack-SmartSearchNotebookRole6F6BB12B-CJB0XY0C4O91\n",
      "sagemaker bucket: sagemaker-us-east-1-249517808360\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "\n",
    "# you can use your own bucket name\n",
    "sagemaker_session_bucket=None\n",
    "\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "# sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdf8ce1-09ef-4221-a53b-3ba4e14c928e",
   "metadata": {},
   "source": [
    "## Fine-tune LLaMA 2 on Amazon SageMaker\n",
    "optimizations ways for efficient fine-tuning:\n",
    "\n",
    "- Low-Rank Adaptation (LoRA) – This is a type of parameter efficient fine-tuning (PEFT) for efficient fine-tuning of large models. In this, we freeze the whole model and only add a small set of adjustable parameters or layers into the model. For instance, instead of training all 7 billion parameters for Llama 2 7B, we can fine-tune less than 1% of the parameters. This helps in significant reduction of the memory requirement because we only need to store gradients, optimizer states, and other training-related information for only 1% of the parameters. Furthermore, this helps in reduction of training time as well as the cost. For more details on this method, refer to LoRA: Low-Rank Adaptation of Large Language Models.\n",
    "- Int8 quantization – Even with optimizations such as LoRA, models such as Llama 70B are still too big to train. To decrease the memory footprint during training, we can use Int8 quantization during training. Quantization typically reduces the precision of the floating point data types. Although this decreases the memory required to store model weights, it degrades the performance due to loss of information. Int8 quantization uses only a quarter precision but doesn’t incur degradation of performance because it doesn’t simply drop the bits. It rounds the data from one type to the another. To learn about Int8 quantization, refer to LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale.\n",
    "- Fully Sharded Data Parallel (FSDP) – This is a type of data-parallel training algorithm that shards the model’s parameters across data parallel workers and can optionally offload part of the training computation to the CPUs. Although the parameters are sharded across different GPUs, computation of each microbatch is local to the GPU worker. It shards parameters more uniformly and achieves optimized performance via communication and computation overlapping during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699ff2ad-b4d5-413a-8744-f999e63a09b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 数据集准备示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4115a781-1ad5-4c28-aa03-cfb91c1b0d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dolly_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "# To train for question answering/information extraction, you can replace the assertion in next line to example[\"category\"] == \"closed_qa\"/\"information_extraction\".\n",
    "summarization_dataset = dolly_dataset.filter(lambda example: example[\"category\"] == \"summarization\")\n",
    "summarization_dataset = summarization_dataset.remove_columns(\"category\")\n",
    "\n",
    "# We split the dataset into two where test data is used to evaluate at the end.\n",
    "train_and_test_dataset = summarization_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# Dumping the training data to a local file to be used for training.\n",
    "train_and_test_dataset[\"train\"].to_json(\"train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eef1a2a-ef58-411f-ac95-3fdab22d5f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617fd2cd-9168-498f-8a38-d8df91da6491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reate a prompt template\n",
    "import json\n",
    "\n",
    "template = {\n",
    "    \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\\n\\n\",\n",
    "    \"completion\": \" {response}\",\n",
    "}\n",
    "with open(\"template.json\", \"w\") as f:\n",
    "    json.dump(template, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a69575a-9688-4f8b-aea3-41dccc48f34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload to s3\n",
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import random\n",
    "\n",
    "output_bucket = sagemaker.Session().default_bucket()\n",
    "local_data_file = \"train.jsonl\"\n",
    "train_data_location = f\"s3://{output_bucket}/dolly_dataset\"\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "S3Uploader.upload(\"template.json\", train_data_location)\n",
    "print(f\"Training data: {train_data_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cca5494-fad2-43f5-b23c-9994f6470317",
   "metadata": {},
   "source": [
    "#### 训练实例类型选择参考：\n",
    "\n",
    "\n",
    "| Model        | Instance Type     | Max Batch Size | Context Length |\n",
    "|--------------|-------------------|----------------|----------------|\n",
    "| [LLama 7B]() | `(ml.)g5.4xlarge` | `3`            | `2048`         |\n",
    "| [LLama 13B]() | `(ml.)g5.4xlarge` | `2`            | `2048`         |\n",
    "| [LLama 70B]() | `(ml.)p4d.24xlarge` | `1++` (need to test more configs)            | `2048`         |\n",
    "\n",
    "\n",
    "> You can also use `g5.2xlarge` instead of the `g5.4xlarge` instance type, but then it is not possible to use `merge_weights` parameter, since to merge the LoRA weights into the model weights, the model needs to fit into memory. But you could save the adapter weights and merge them using [merge_adapter_weights.py](./scripts/merge_adapter_weights.py) after training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9421280-6fd9-41cc-9897-fa910c1a0a4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Fine-tune via the Hugging Face SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256331d9-b79f-44ac-9168-9b0b98edf8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-qlora-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                             # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training',    # path where sagemaker will save training dataset\n",
    "  'epochs': 3,                                      # number of training epochs\n",
    "  'per_device_train_batch_size': 2,                 # batch size for training\n",
    "  'lr': 2e-4,                                       # learning rate used during training\n",
    "  'hf_token': HfFolder.get_token(),                 # huggingface token to access llama 2\n",
    "  'merge_weights': True,                            # wether to merge LoRA into the model (needs more memory)\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_clm.py',      # train script\n",
    "    source_dir           = 'scripts',         # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.4xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa25b44f-a0f2-4296-8243-ae2bf942f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6f78cd-57aa-4add-a0b9-37b9a0e39a06",
   "metadata": {},
   "source": [
    "#### Fine-tune via the SageMaker Python SDK\n",
    "more training  [sample code](https://github.com/aws/amazon-sagemaker-examples/tree/5daada592797be296c1aa7e7964e73900473dddd/introduction_to_amazon_algorithms/jumpstart-foundation-models#6.-Studio-Kernel-Dead/Creating-JumpStart-Model-from-the-training-Job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ac476b-0d4d-49b8-8041-4270d85bebf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "model_id, model_version = \"meta-textgeneration-llama-2-7b\", \"*\"\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    environment={\"accept_eula\": \"true\"},\n",
    "    disable_output_compression=True,  # For Llama-2-70b, add instance_type = \"ml.g5.48xlarge\"\n",
    ")\n",
    "# By default, instruction tuning is set to false. Thus, to use instruction tuning dataset you use\n",
    "estimator.set_hyperparameters(instruction_tuned=\"True\", epoch=\"5\", max_input_length=\"1024\")\n",
    "estimator.fit({\"training\": train_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b36cd4-717f-448a-8008-799f258935c1",
   "metadata": {},
   "source": [
    "Find the training job name:\n",
    "Go to Console -> SageMaker -> Training -> Training Jobs -> Identify the training job name and substitute in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a9958f-d99f-4b6d-be9c-d81246051f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果kernel 断开之后通过training jod 新建estimator对象\n",
    "'''\n",
    "# from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "# training_job_name = <<training_job_name>>\n",
    "\n",
    "# estimator = JumpStartEstimator.attach(training_job_name, model_id)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5554ca28-3136-4d55-8fe9-ec141cee9bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93c82f7c-31a0-4a64-ba8f-b3bf76189696",
   "metadata": {},
   "source": [
    "## Deploy Llama 2 (7B - 70B) on Amazon SageMaker\n",
    "Main Steps：\n",
    "1. [# Setup development environment](#1-setup-development-environment)\n",
    "2. [Retrieve the new Hugging Face LLM DLC](#2-retrieve-the-new-hugging-face-llm-dlc)\n",
    "3. [Hardware requirements](#3-hardware-requirements)\n",
    "4. [Deploy Llama 2 to Amazon SageMaker](#4-deploy-llama-2-to-amazon-sagemaker)\n",
    "5. [Run inference and chat with the model](#5-run-inference-and-chat-with-the-model)\n",
    "6. [Clean up](#5-clean-up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14e9c40-084d-4812-9c1b-2f4914d1948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you train llama on sagemaker,you can deploy with the estimator defined before\n",
    "'''\n",
    "finetuned_predictor = estimator.deploy()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cacf2b6-d63a-4020-9cbc-a72da2f33189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy pre-trained model\n",
    "'''\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "pretrained_model = JumpStartModel(model_id=model_id)\n",
    "pretrained_predictor = pretrained_model.deploy()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea06e51-37d1-4e20-9dc8-aa8d10aa7d6c",
   "metadata": {},
   "source": [
    "### 自定义推理镜像\n",
    "如果是已经训练好的模型只需要做部署的话，需要指定推理镜像\n",
    "- hugging facing\n",
    "- sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550cfbbc-3d8a-45bc-9ce9-3d52f93070fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"0.9.3\"\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76de609b-cf03-49bf-a523-1313772d13d3",
   "metadata": {},
   "source": [
    "### 部署实例选择参考\n",
    "| Model        | Instance Type     | Quantization | # of GPUs per replica | \n",
    "|--------------|-------------------|--------------|-----------------------|\n",
    "| [Llama 7B](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) | `(ml.)g5.2xlarge` | `-`          | 1 |\n",
    "| [Llama 13B](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf) | `(ml.)g5.12xlarge` | `-`          | 4                     | \n",
    "| [Llama 70B](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf) | `(ml.)g5.48xlarge` | `bitsandbytes`          | 8                     | \n",
    "| [Llama 70B](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf) | `(ml.)p4d.24xlarge` | `-`          | 8                     | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d8681c-4340-4a23-984c-ecdb56a6bd85",
   "metadata": {},
   "source": [
    "### 部署参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c27d61-a5fa-4124-8a0d-30c0e60c695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.p4d.24xlarge\"\n",
    "number_of_gpu = 8\n",
    "health_check_timeout = 300\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"meta-llama/Llama-2-70b-chat-hf\", # model_id from hf.co/models\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(2048),  # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(4096),  # Max length of the generation (including input text)\n",
    "  'MAX_BATCH_TOTAL_TOKENS': json.dumps(8192),  # Limits the number of tokens that can be processed in parallel during the generation\n",
    "  'HUGGING_FACE_HUB_TOKEN': \"<REPLACE WITH YOUR TOKEN>\"\n",
    "  # ,'HF_MODEL_QUANTIZE': \"bitsandbytes\", # comment in to quantize\n",
    "}\n",
    "\n",
    "# check if token is set\n",
    "assert config['HUGGING_FACE_HUB_TOKEN'] != \"<REPLACE WITH YOUR TOKEN>\", \"Please set your Hugging Face Hub token\"\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b667e125-85d5-42c8-aa53-c093293be765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy model to an endpoint\n",
    "# https://sagemaker.llm_modelreadthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "llm = .deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041b97fe-f23e-42b3-85de-d3f21ac79f2d",
   "metadata": {},
   "source": [
    "### 测试部署好的节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baef4cd5-69bd-40cb-8ba0-f79cf081f687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_llama2_prompt(messages):\n",
    "    startPrompt = \"<s>[INST] \"\n",
    "    endPrompt = \" [/INST]\"\n",
    "    conversation = []\n",
    "    for index, message in enumerate(messages):\n",
    "        if message[\"role\"] == \"system\" and index == 0:\n",
    "            conversation.append(f\"<<SYS>>\\n{message['content']}\\n<</SYS>>\\n\\n\")\n",
    "        elif message[\"role\"] == \"user\":\n",
    "            conversation.append(message[\"content\"].strip())\n",
    "        else:\n",
    "            conversation.append(f\" [/INST] {message['content'].strip()} </s><s>[INST] \")\n",
    "\n",
    "    return startPrompt + \"\".join(conversation) + endPrompt\n",
    "  \n",
    "messages = [\n",
    "  { \"role\": \"system\",\"content\": \"You are a friendly and knowledgeable vacation planning assistant named Clara. Your goal is to have natural conversations with users to help them plan their perfect vacation. \"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9a9a4b-95cd-4785-8bb4-38b0844021d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define question and add to messages\n",
    "instruction = \"What are some cool ideas to do in the summer?\"\n",
    "messages.append({\"role\": \"user\", \"content\": instruction})\n",
    "prompt = build_llama2_prompt(messages)\n",
    "\n",
    "\n",
    "chat = llm.predict({\"inputs\":prompt})\n",
    "\n",
    "print(chat[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd12e30-8d81-4c36-86ff-fd6323a39f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for llm\n",
    "payload = {\n",
    "  \"inputs\":  prompt,\n",
    "  \"parameters\": {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "    \"stop\": [\"</s>\"]\n",
    "  }\n",
    "}\n",
    "\n",
    "# send request to endpoint\n",
    "response = llm.predict(payload)\n",
    "\n",
    "print(response[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c250d219-6cf4-47d8-b571-954f4f047003",
   "metadata": {},
   "source": [
    "### 使用Python调用SageMaker端点\n",
    "invoke_endpoint方法会发送HTTP请求到指定的端点,并返回响应。重要参数有:\n",
    "- EndpointName: 要调用的端点名称\n",
    "- Body: 要发送的数据\n",
    "- ContentType: Body数据的内容类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "296e95c5-8e2d-496d-9c5f-9670b04c263b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf40a032-9039-4216-bead-2f29ef605ac4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_client = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e14115-7c63-4b3d-a188-1f1f2c102209",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sagemaker_client.invoke_endpoint(\n",
    "    EndpointName='your-endpoint-name',\n",
    "    Body=payload,\n",
    "    ContentType='application/json'\n",
    ")\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "print(result) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01884cf-cae2-41d5-b667-d39537fb4b41",
   "metadata": {},
   "source": [
    "## 测试结束需要及时删除推理节点\n",
    "Go to Console -> SageMaker -> Inference -> Endpoint -> Identify the test endpoint name and then del it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa75bd7-378d-471e-927a-b82dd1de37ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
